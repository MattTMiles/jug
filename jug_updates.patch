diff --git a/docs/TESTING.md b/docs/TESTING.md
index 06efc66..de1d9f7 100644
--- a/docs/TESTING.md
+++ b/docs/TESTING.md
@@ -8,32 +8,82 @@ Quick start for running tests and validating functionality.
 # From repo root, run all tests
 python tests/run_all.py
 
-# Quick validation (skip slow tests)
-python tests/run_all.py --quick
+# Quick validation (no external data, skip slow tests, no GUI)
+python tests/run_all.py --quick --no-gui
+
+# Full validation including GUI
+python tests/run_all.py
+
+# With PINT cross-validation
+python tests/run_all.py --pint
 
 # Verbose output for debugging
 python tests/run_all.py -v
 
 # Run specific tests
 python tests/run_all.py imports prebinary_cache
+
+# Run only a category
+python tests/run_all.py -c api
+python tests/run_all.py -c correctness
+
+# List all available tests
+python tests/run_all.py --list
 ```
 
+## Test Categories
+
+| Category | Description | Data Required | GUI Required |
+|----------|-------------|---------------|--------------|
+| `critical` | Must pass - core imports | No | No |
+| `cli` | CLI smoke tests | No | No |
+| `api` | Python API workflow | No (uses mini) | No |
+| `correctness` | Golden reference validation | No (uses mini) | No |
+| `gui` | GUI initialization tests | No | Yes |
+| `standard` | Standard validation tests | Yes | No |
+| `slow` | Long-running tests | Yes | No |
+
 ## What the Tests Check
 
-| Test | Purpose | Duration |
-|------|---------|----------|
-| `imports` | Core module imports | <1s |
-| `prebinary_cache` | Cache path regression | ~2s |
-| `timescale_validation` | TDB/TCB handling | ~2s |
-| `binary_patch` | Binary delay correctness | ~3s |
-| `astrometry_fitting` | Astrometry parameters | ~4s |
-| `j2241_fit` | Full parameter fitting | ~3s |
+| Test | Category | Purpose | Duration |
+|------|----------|---------|----------|
+| `imports` | critical | Core module imports | <1s |
+| `prebinary_cache` | critical | Cache path regression | ~2s |
+| `cli_smoke` | cli | CLI entry points work | ~3s |
+| `api_workflow` | api | Python API with bundled data | ~2s |
+| `correctness` | correctness | Residuals match golden values | ~2s |
+| `gui_smoke` | gui | GUI initializes headless | ~3s |
+| `timescale_validation` | standard | TDB/TCB handling | ~2s |
+| `binary_patch` | standard | Binary delay correctness | ~3s |
+| `astrometry_fitting` | standard | Astrometry parameters | ~4s |
+| `j2241_fit` | slow | Full parameter fitting | ~3s |
+
+## Quick Mode (CI-Friendly)
+
+Use `--quick --no-gui` for fast CI validation without external data:
+
+```bash
+python tests/run_all.py --quick --no-gui
+```
+
+This runs:
+- Import tests
+- CLI smoke tests  
+- API workflow tests (uses bundled mini data)
+- Correctness tests (uses bundled mini data)
+
+## Bundled Test Data
+
+The `tests/data_golden/` directory contains:
+- `J1909_mini.par` - Simplified par file (20 TOAs)
+- `J1909_mini.tim` - Mini tim file (20 TOAs)
+- `J1909_mini_golden.json` - Golden reference values
 
-Use `python tests/run_all.py --quick` to skip `j2241_fit`.
+These enable CI tests to run without external data dependencies.
 
 ## CI/Portable Test Data
 
-Tests auto-skip if data is missing. To run on CI/other machines, set:
+For external data tests, set environment variables:
 
 ```bash
 export JUG_TEST_DATA_DIR=/path/to/data
@@ -52,6 +102,27 @@ Check your setup:
 python tests/test_paths.py
 ```
 
+## GitHub Actions
+
+The `.github/workflows/tests.yml` workflow runs:
+
+1. **Quick tests** - Every push, Python 3.10/3.11/3.12, no external data
+2. **Full tests** - On commits containing `[full-tests]`, includes GUI
+3. **PINT validation** - On PRs, cross-validates against PINT
+4. **Lint** - Code quality checks with ruff/black
+
+## Correctness Validation
+
+JUG validates correctness by comparing computed residuals against:
+
+1. **Golden reference** - Pre-computed values in `tests/data_golden/`
+2. **PINT (optional)** - Cross-validation with `--pint` flag
+
+To regenerate golden values after intentional changes:
+```bash
+python tests/generate_golden.py
+```
+
 ## Debug Scripts
 
 Debug/diagnostic scripts are in `playground/`. Run manually as needed:
diff --git a/tests/run_all.py b/tests/run_all.py
index 8116870..7645569 100644
--- a/tests/run_all.py
+++ b/tests/run_all.py
@@ -4,16 +4,23 @@ JUG Test Runner - One-command test execution.
 
 Run from repo root:
     python tests/run_all.py           # Run all tests
-    python tests/run_all.py --quick   # Run quick tests only (skip slow)
+    python tests/run_all.py --quick   # Run quick tests only (skip slow, no external data)
+    python tests/run_all.py --full    # Run all tests including optional PINT validation
+    python tests/run_all.py --no-gui  # Skip GUI tests (for headless CI)
     python tests/run_all.py -v        # Verbose output
 
 This runner executes script-style tests in a sensible order and provides
 a concise PASS/FAIL summary. It exits nonzero on any failure.
 
-Tests are categorized as:
+Test categories:
 - CRITICAL: Must pass (failures are hard errors)
 - STANDARD: Should pass (failures are reported)
 - SLOW: Take longer to run (skipped with --quick)
+- CLI: Command-line interface smoke tests
+- API: Python API workflow tests
+- GUI: GUI tests (require Qt, skip with --no-gui)
+- CORRECTNESS: Golden reference validation
+- DATA_REQUIRED: Need external data files (skipped with --quick)
 
 Environment variables for CI:
     JUG_TEST_DATA_DIR=/path/to/data   # Base directory for test data
@@ -40,9 +47,11 @@ class TestSpec:
     """Specification for a single test."""
     name: str
     script: str
-    category: str = "standard"  # critical, standard, slow
+    category: str = "standard"  # critical, standard, slow, cli, api, gui, correctness
     description: str = ""
     timeout: int = 120  # seconds
+    requires_data: bool = False  # True if needs external data files
+    requires_gui: bool = False   # True if needs Qt/display
 
 
 # Tests in execution order
@@ -61,24 +70,60 @@ TESTS = [
         description="Regression: prebinary_delay_sec in cache path",
     ),
     
-    # Standard tests
+    # CLI smoke tests
+    TestSpec(
+        name="cli_smoke",
+        script="test_cli_smoke.py",
+        category="cli",
+        description="CLI entry points respond to --help",
+    ),
+    
+    # API tests (use bundled mini data, no external deps)
+    TestSpec(
+        name="api_workflow",
+        script="test_api_workflow.py",
+        category="api",
+        description="Python API workflow with bundled data",
+    ),
+    
+    # Correctness tests (use bundled mini data)
+    TestSpec(
+        name="correctness",
+        script="test_correctness.py",
+        category="correctness",
+        description="Residuals match golden reference",
+    ),
+    
+    # GUI tests (need Qt)
+    TestSpec(
+        name="gui_smoke",
+        script="test_gui_smoke.py",
+        category="gui",
+        description="GUI initializes headless",
+        requires_gui=True,
+    ),
+    
+    # Standard tests (need external data)
     TestSpec(
         name="timescale_validation",
         script="test_timescale_validation.py",
         category="standard",
         description="Par file timescale (TDB/TCB) handling",
+        requires_data=True,
     ),
     TestSpec(
         name="binary_patch",
         script="test_binary_patch.py",
         category="standard",
         description="Binary delay patch vs PINT",
+        requires_data=True,
     ),
     TestSpec(
         name="astrometry_fitting",
         script="test_astrometry_fitting.py",
         category="standard",
         description="Astrometry parameter fitting",
+        requires_data=True,
     ),
     
     # Slow tests
@@ -88,6 +133,7 @@ TESTS = [
         category="slow",
         description="J2241-5236 FB parameter fitting",
         timeout=180,
+        requires_data=True,
     ),
 ]
 
@@ -285,7 +331,27 @@ def main():
     parser.add_argument(
         "--quick", "-q",
         action="store_true",
-        help="Skip slow tests"
+        help="Skip slow tests and tests requiring external data"
+    )
+    parser.add_argument(
+        "--full", "-f",
+        action="store_true",
+        help="Run all tests including slow and optional PINT validation"
+    )
+    parser.add_argument(
+        "--no-gui",
+        action="store_true",
+        help="Skip GUI tests (for headless CI)"
+    )
+    parser.add_argument(
+        "--data-required",
+        action="store_true",
+        help="Only run tests that require external data files"
+    )
+    parser.add_argument(
+        "--pint",
+        action="store_true",
+        help="Include PINT cross-validation in correctness tests"
     )
     parser.add_argument(
         "--verbose", "-v",
@@ -297,6 +363,11 @@ def main():
         action="store_true",
         help="List available tests and exit"
     )
+    parser.add_argument(
+        "--category", "-c",
+        choices=["critical", "standard", "slow", "cli", "api", "gui", "correctness"],
+        help="Run only tests in this category"
+    )
     parser.add_argument(
         "tests",
         nargs="*",
@@ -308,28 +379,63 @@ def main():
     if args.list:
         print("Available tests:")
         for spec in TESTS:
-            skip_marker = " [slow]" if spec.category == "slow" else ""
-            crit_marker = " [critical]" if spec.category == "critical" else ""
-            print(f"  {spec.name}{crit_marker}{skip_marker}: {spec.description}")
+            markers = []
+            if spec.category == "slow":
+                markers.append("slow")
+            if spec.category == "critical":
+                markers.append("critical")
+            if spec.requires_data:
+                markers.append("data-required")
+            if spec.requires_gui:
+                markers.append("gui")
+            marker_str = f" [{', '.join(markers)}]" if markers else ""
+            print(f"  {spec.name} ({spec.category}){marker_str}: {spec.description}")
         return 0
     
     # Filter tests
+    tests_to_run = TESTS.copy()
+    
+    # Filter by specific test names
     if args.tests:
         test_names = set(args.tests)
-        tests_to_run = [t for t in TESTS if t.name in test_names]
+        tests_to_run = [t for t in tests_to_run if t.name in test_names]
         if not tests_to_run:
             print(f"ERROR: No matching tests found for: {args.tests}")
             return 1
-    elif args.quick:
-        tests_to_run = [t for t in TESTS if t.category != "slow"]
-    else:
-        tests_to_run = TESTS
+    
+    # Filter by category
+    if args.category:
+        tests_to_run = [t for t in tests_to_run if t.category == args.category]
+    
+    # Quick mode: skip slow and data-requiring tests
+    if args.quick:
+        tests_to_run = [t for t in tests_to_run if t.category != "slow" and not t.requires_data]
+    
+    # Skip GUI tests if requested
+    if args.no_gui:
+        tests_to_run = [t for t in tests_to_run if not t.requires_gui]
+    
+    # Only data-required tests
+    if args.data_required:
+        tests_to_run = [t for t in tests_to_run if t.requires_data]
+    
+    # Set PINT flag for correctness tests
+    if args.pint:
+        os.environ['JUG_TEST_PINT'] = '1'
     
     # Run tests
     print("=" * 60)
     print("JUG Test Runner")
     print("=" * 60)
-    print(f"\nRunning {len(tests_to_run)} tests...")
+    mode_info = []
+    if args.quick:
+        mode_info.append("quick")
+    if args.no_gui:
+        mode_info.append("no-gui")
+    if args.pint:
+        mode_info.append("+pint")
+    mode_str = f" ({', '.join(mode_info)})" if mode_info else ""
+    print(f"\nRunning {len(tests_to_run)} tests{mode_str}...")
     
     results: List[TestResult] = []
     start_time = time.time()
diff --git a/tests/test_paths.py b/tests/test_paths.py
index 2b20490..b813580 100644
--- a/tests/test_paths.py
+++ b/tests/test_paths.py
@@ -137,6 +137,38 @@ def get_j1022_paths() -> Tuple[Optional[Path], Optional[Path]]:
     )
 
 
+def get_mini_paths() -> Tuple[Path, Path]:
+    """Get bundled J1909_mini PAR/TIM paths.
+    
+    These are always available (bundled in tests/data_golden/) and require
+    no external data files. Used for CI quick tests.
+    
+    Returns:
+        Tuple of (par_path, tim_path). These always exist.
+    """
+    golden_dir = Path(__file__).parent / "data_golden"
+    par = golden_dir / "J1909_mini.par"
+    tim = golden_dir / "J1909_mini.tim"
+    return par, tim
+
+
+def get_golden_reference(name: str = "J1909_mini") -> Optional[dict]:
+    """Load golden reference values from JSON.
+    
+    Args:
+        name: Dataset name (default: J1909_mini)
+        
+    Returns:
+        Dictionary with golden values, or None if not found.
+    """
+    import json
+    golden_file = Path(__file__).parent / "data_golden" / f"{name}_golden.json"
+    if not golden_file.exists():
+        return None
+    with open(golden_file) as f:
+        return json.load(f)
+
+
 def files_exist(par: Optional[Path], tim: Optional[Path]) -> bool:
     """Check if both PAR and TIM files exist."""
     if par is None or tim is None:
